{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyNe1GnOyFIhw0JK7CEA9qRg"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Text Normalization\n",
        "\n",
        "by Michael Hunziker\n",
        "\n",
        "## Summary\n",
        "In this notebook we tokenize, stemm and lemmanize our data in order to prepare it as training data for a downstream nlp task.\n",
        "Then we visualize the data in order to decide upon reasonable stopwords and identify malformed and outlier data that should be removed.\n",
        "And we optimize for tweets.\n",
        "\n",
        "\n",
        "*   Tokenize, stemm and lemmanize our data\n",
        "*   Identify reasonable stopwords\n",
        "*   Optimize for tweets\n",
        "*   Save the normalized version\n",
        "\n",
        "</br>\n",
        "\n",
        "<a href=\"https://colab.research.google.com/github/miam-bonbon/assignment-adv-nlp/blob/main/adv_nlp_assignment_mh_02_text_normalization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ],
      "metadata": {
        "id": "QvQ3iJJnU9dg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bu035c5DTxq7"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "\n",
        "!pip install 'fhnw-nlp-utils>=0.8.0,<0.9.0'\n",
        "\n",
        "from fhnw.nlp.utils.processing import parallelize_dataframe\n",
        "from fhnw.nlp.utils.processing import is_iterable\n",
        "from fhnw.nlp.utils.storage import download\n",
        "from fhnw.nlp.utils.storage import save_dataframe\n",
        "from fhnw.nlp.utils.storage import load_dataframe\n",
        "from fhnw.nlp.utils.text import join_tokens\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from fhnw.nlp.utils.system import set_log_level\n",
        "from fhnw.nlp.utils.system import system_info\n",
        "\n",
        "set_log_level()\n",
        "print(system_info())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YgtxeSJgnBDX",
        "outputId": "09f3bead-70a1-4595-9d19-6b415fac353f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OS name: posix\n",
            "Platform name: Linux\n",
            "Platform release: 6.1.85+\n",
            "Python version: 3.10.12\n",
            "CPU brand: AMD EPYC 7B12\n",
            "CPU cores: 4\n",
            "RAM: 50.99GB total and 49.58GB available\n",
            "Tensorflow version: 2.17.1\n",
            "GPU is NOT AVAILABLE\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create a DEV variable to use later\n",
        "DEV = False"
      ],
      "metadata": {
        "id": "kP4pPMBD4dFR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "!rm \"./data/COVIDSenti_cleaned.parq\"\n",
        "\n",
        "download(\"https://github.com/miam-bonbon/assignment-adv-nlp/raw/refs/heads/main/data/COVIDSenti_cleaned.parq\", \"data/COVIDSenti_cleaned.parq\")\n",
        "data = load_dataframe(\"data/COVIDSenti_cleaned.parq\")\n",
        "data.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0uQNs9wdnHPM",
        "outputId": "ac44cc69-9ce1-4827-a62a-39f4f4d565ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rm: cannot remove './data/COVIDSenti_cleaned.parq': No such file or directory\n",
            "CPU times: user 452 ms, sys: 62.5 ms, total: 515 ms\n",
            "Wall time: 2.46 s\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(22516, 4)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data.head(3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "id": "bxlqZFgTnVOf",
        "outputId": "b9ce7148-6231-4fd4-aef8-4b956fe6ec6a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                tweet label  \\\n",
              "2   TLDR: Not SARS, possibly new coronavirus. Diff...   neg   \n",
              "8   @tezuma75 Why #CCP keep on saying unknown caus...   neg   \n",
              "11  I always feel weird hoping for another coronav...   neg   \n",
              "\n",
              "                                        cleaned_tweet lang  \n",
              "2   TLDR: Not SARS, possibly new coronavirus. Diff...   en  \n",
              "8    Why #CCP keep on saying unknown cause of pneu...   en  \n",
              "11  I always feel weird hoping for another coronav...   en  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-2aa94944-a7a2-4b37-ad93-0fe04e0f4d4d\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweet</th>\n",
              "      <th>label</th>\n",
              "      <th>cleaned_tweet</th>\n",
              "      <th>lang</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>TLDR: Not SARS, possibly new coronavirus. Diff...</td>\n",
              "      <td>neg</td>\n",
              "      <td>TLDR: Not SARS, possibly new coronavirus. Diff...</td>\n",
              "      <td>en</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>@tezuma75 Why #CCP keep on saying unknown caus...</td>\n",
              "      <td>neg</td>\n",
              "      <td>Why #CCP keep on saying unknown cause of pneu...</td>\n",
              "      <td>en</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>I always feel weird hoping for another coronav...</td>\n",
              "      <td>neg</td>\n",
              "      <td>I always feel weird hoping for another coronav...</td>\n",
              "      <td>en</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2aa94944-a7a2-4b37-ad93-0fe04e0f4d4d')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-2aa94944-a7a2-4b37-ad93-0fe04e0f4d4d button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-2aa94944-a7a2-4b37-ad93-0fe04e0f4d4d');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-5811cc37-a806-472d-9d6d-7076998f457e\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-5811cc37-a806-472d-9d6d-7076998f457e')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-5811cc37-a806-472d-9d6d-7076998f457e button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "data",
              "summary": "{\n  \"name\": \"data\",\n  \"rows\": 22516,\n  \"fields\": [\n    {\n      \"column\": \"tweet\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 22449,\n        \"samples\": [\n          \"\\\"Politics, coronavirus, why me? I wear a baseball cap and have a bad shave.\\\" If Jurgen Klopp told me to rub myself\\u201a\\u00c4\\u00b6 https://t.co/jmclDqR8wA\",\n          \"East African airlines Kenya Airways and Rwandair suspend China flights due to coronavirus. \\uf8ff\\u00fc\\u00e1\\u221e\\uf8ff\\u00fc\\u00e1\\u2122\\uf8ff\\u00fc\\u00e1\\u2211\\uf8ff\\u00fc\\u00e1\\u00ba\\uf8ff\\u00fc\\u00e1\\u00ae\\uf8ff\\u00fc\\u00e1\\u2265\\uf8ff\\u00fc\\u00f5\\u00a8#KenyaAirways\\u201a\\u00c4\\u00b6 https://t.co/zRjvFr6A8w\",\n          \"BREAKING: Democrats claim Trump is too \\\"afraid\\\" to get Coronavirus. Wait patiently.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"label\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"pos\",\n          \"neg\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"cleaned_tweet\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 21741,\n        \"samples\": [\n          \"Series Mania TV festival and industry event cancelled due to coronavirus \",\n          \"A -year-old Americans tense trip through China, fleeing the coronavirus. I took my mask off for five seconds an \"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"lang\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"en\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define our normalization functions (could we use the ones from the library? We'll check later)"
      ],
      "metadata": {
        "id": "jb4uD2trr_Y2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def _tokenize(text, stopwords):\n",
        "    \"\"\"Tokenizes and lowercases a text and removes stopwords\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    text : str, iterable\n",
        "        The text either as string or iterable of tokens (in this case tokenization is not applied)\n",
        "    stopwords : set\n",
        "        A set of stopword to remove from the tokens\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    list\n",
        "        The tokenized text\n",
        "    \"\"\"\n",
        "    from fhnw.nlp.utils.processing import is_iterable\n",
        "\n",
        "    if isinstance(text, str):\n",
        "        from nltk.tokenize import word_tokenize\n",
        "        word_tokens = word_tokenize(text)\n",
        "    elif is_iterable(text):\n",
        "        word_tokens = text\n",
        "    else:\n",
        "        raise TypeError(\"Only string or iterable (e.g. list) is supported. Received a \"+ str(type(text)))\n",
        "\n",
        "    return [word.lower() for word in word_tokens if word.lower() not in stopwords]\n",
        "\n",
        "\n",
        "def _tokenize_stem(text, stopwords, stemmer):\n",
        "    \"\"\"Tokenizes, lowercases and stems a text and removes stopwords\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    text : str, iterable\n",
        "        The text either as string or iterable of tokens (in this case tokenization is not applied)\n",
        "    stopwords : set\n",
        "        A set of stopword to remove from the tokens\n",
        "    stemmer: stemmer\n",
        "        The stemmer to use (e.g. SnowballStemmer)\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    list\n",
        "        The tokenized and stemmed text\n",
        "    \"\"\"\n",
        "    from fhnw.nlp.utils.processing import is_iterable\n",
        "\n",
        "    if isinstance(text, str):\n",
        "        from nltk.tokenize import word_tokenize\n",
        "        word_tokens = word_tokenize(text)\n",
        "    elif is_iterable(text):\n",
        "        word_tokens = text\n",
        "    else:\n",
        "        raise TypeError(\"Only string or iterable (e.g. list) is supported. Received a \"+ str(type(text)))\n",
        "\n",
        "    # if not isinstance(stemmer, Callable):\n",
        "    stemmer = stemmer.stem\n",
        "\n",
        "    return [stemmer(word.lower()) for word in word_tokens if word.lower() not in stopwords]\n",
        "\n",
        "\n",
        "def _tokenize_lemma(text, stopwords, lemmanizer, keep_ners=False):\n",
        "    \"\"\"Tokenizes, lowercases and lemmatizes a text and removes stopwords\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    text : str, iterable\n",
        "        The text either as string or iterable of tokens (in this case tokenization is not applied)\n",
        "    stopwords : set\n",
        "        A set of stopword to remove from the tokens\n",
        "    lemmanizer: spacy nlp pipeline\n",
        "        The lemmanizer to use (must be spacy nlp pipeline)\n",
        "    keep_ner: bool\n",
        "        Defines if named entities (NERs) should be keept in one token\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    list\n",
        "        The tokenized and lemmatized text\n",
        "    \"\"\"\n",
        "    from fhnw.nlp.utils.processing import is_iterable\n",
        "\n",
        "    if isinstance(text, str):\n",
        "        text = text\n",
        "    elif is_iterable(text):\n",
        "        from fhnw.nlp.utils.text import join_tokens\n",
        "        text = join_tokens(text, set())\n",
        "    else:\n",
        "        raise TypeError(\"Only string or iterable (e.g. list) is supported. Received a \"+ str(type(text)))\n",
        "\n",
        "    if keep_ners:\n",
        "        # HanoverTagger could be an alternative but takes longer\n",
        "        # see: https://textmining.wp.hs-hannover.de/Preprocessing.html#Lemmatisierung-und-Wortarterkennung\n",
        "        doc = lemmanizer(text, disable=['tagger', 'parser'])\n",
        "\n",
        "        tokens = list()\n",
        "        ner_idx = 0\n",
        "        tok_idx = 0\n",
        "\n",
        "        # keep ner in one token\n",
        "        while tok_idx < len(doc):\n",
        "            if ner_idx >= len(doc.ents) or doc[tok_idx].idx < doc.ents[ner_idx].start_char:\n",
        "                if doc[tok_idx].is_alpha and not doc[tok_idx].is_punct and doc[tok_idx].text.lower() not in stopwords and doc[tok_idx].lemma_.lower() not in stopwords:\n",
        "                    #print(\"token \", doc[tok_idx].lemma_.lower())\n",
        "                    tokens.append(doc[tok_idx].lemma_.lower())\n",
        "\n",
        "                tok_idx += 1\n",
        "            else:\n",
        "                #print(\"ner \", doc.ents[ner_idx].lemma_.lower())\n",
        "                tokens.append(doc.ents[ner_idx].lemma_.lower())\n",
        "\n",
        "                tok_idx += 1\n",
        "                while tok_idx < len(doc) and doc[tok_idx].idx < doc.ents[ner_idx].end_char:\n",
        "                    tok_idx += 1\n",
        "\n",
        "                ner_idx += 1\n",
        "\n",
        "        return tokens\n",
        "    else:\n",
        "        doc = lemmanizer(text, disable=['tagger', 'parser', 'ner'])\n",
        "        return [tok.lemma_.lower() for tok in doc if tok.is_alpha and not tok.is_punct and tok.text.lower() not in stopwords and tok.lemma_.lower() not in stopwords]\n",
        "\n",
        "\n",
        "def _normalize(text, stopwords, stemmer=None, lemmanizer=None, lemma_with_ner=False):\n",
        "    \"\"\"Normalizes (e.g. tokenize and stem) and lowercases a text and removes stopwords\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    text : str, iterable\n",
        "        The text either as string or iterable of tokens (in this case tokenization is not applied)\n",
        "    stopwords : set\n",
        "        A set of stopword to remove from the tokens\n",
        "    stemmer: stemmer\n",
        "        The stemmer to use (e.g. SnowballStemmer) or None to disable stemming\n",
        "    lemmanizer: spacy nlp pipeline\n",
        "        The lemmanizer to use (must be spacy nlp pipeline) or None to disable lemmantization\n",
        "    lemma_with_ner: bool\n",
        "        Defines if named entities (NERs) should be keept in one token\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    list\n",
        "        The normalized text\n",
        "    \"\"\"\n",
        "\n",
        "    if lemmanizer is not None:\n",
        "        return _tokenize_lemma(text, stopwords, lemmanizer, keep_ners=lemma_with_ner)\n",
        "    elif stemmer is not None:\n",
        "        return _tokenize_stem(text, stopwords, stemmer)\n",
        "    else:\n",
        "        return _tokenize(text, stopwords)"
      ],
      "metadata": {
        "id": "daXneW3LpmEy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's use a spacy model (https://spacy.io/models/en#en_core_web_md)"
      ],
      "metadata": {
        "id": "inb52oZlte9U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "\n",
        "!pip install 'spacy>=3.0.5'\n",
        "!pip install nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "\n",
        "import spacy\n",
        "!python3 -m spacy download en_core_web_md\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_md\")\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "stemmer = SnowballStemmer(\"english\")\n",
        "stopwords = set(stopwords.words(\"english\"))\n",
        "#stopwords = set(nlp.Defaults.stop_words)\n",
        "empty_stopwords = set()"
      ],
      "metadata": {
        "id": "H4YtAPK1smtq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(stopwords)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VejAmxI8CUra",
        "outputId": "69e50656-15b9-4add-da24-9287ac502555"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'by', 'both', 'wouldn', 't', 'some', 'not', 'the', 're', 'won', \"shan't\", 'to', 'below', 'any', \"weren't\", 'more', 'should', 'under', 'd', 'weren', 'itself', 'be', \"hadn't\", 'hers', 'when', \"haven't\", 'into', 'being', 'further', \"wouldn't\", 'doesn', 'no', 'than', 'just', 'against', 'yourselves', 'you', 'him', 'having', 'after', 'yourself', 'whom', 'isn', 'will', \"needn't\", 'a', 'can', 'most', 'don', 'what', \"doesn't\", 'it', 'them', 'there', 'as', 'while', \"that'll\", 'shan', 'because', 's', 'so', \"it's\", 'at', 'then', 'nor', 'down', 'out', \"couldn't\", 'same', 'didn', 'have', 'had', 'of', 'ours', 'am', 'very', 'each', 'wasn', 'she', 'hasn', 'do', 'but', 'ourselves', \"you're\", \"aren't\", \"you'll\", 'y', 'themselves', 'their', 'once', 'doing', \"shouldn't\", \"you've\", 'between', \"should've\", 'only', 'ma', 'that', 'over', 'needn', 'me', 'these', 'm', 'll', 'for', \"you'd\", 'they', 'its', 'o', 'own', 'again', \"mustn't\", 'on', 'now', 'how', 'our', \"don't\", 'hadn', 'haven', \"hasn't\", 'or', 'until', 'did', \"won't\", 'her', 'all', 'theirs', \"wasn't\", \"she's\", 'with', 'few', 'your', 've', 'ain', 'other', 'up', 'himself', 'has', 'too', 'aren', 'mustn', 'here', 'this', 'above', 'were', 'during', 'an', 'couldn', 'my', 'through', 'he', 'i', 'mightn', 'in', 'yours', 'before', 'which', 'those', 'if', 'been', 'myself', 'from', 'shouldn', 'off', 'where', 'his', 'does', 'we', \"didn't\", \"mightn't\", 'herself', 'was', 'who', 'are', 'why', 'and', 'about', 'is', \"isn't\", 'such'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's test some examples:"
      ],
      "metadata": {
        "id": "mevri7EAt1Wj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(_tokenize(\"Coronavirus is that when youve had too many #Coronas \", stopwords=stopwords))\n",
        "print(_tokenize_stem(\"The Frieman Scary Scale makes its debut. Wuhan Coronavirus scores  out of  N masks.\", stopwords=stopwords, stemmer=stemmer))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 669
        },
        "id": "6BE1CCCft2gY",
        "outputId": "36bc7bfe-e157-48ce-a1b7-37b9ae028787"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "LookupError",
          "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-fe8e59cd57e8>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Coronavirus is that when youve had too many #Coronas \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstopwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_tokenize_stem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"The Frieman Scary Scale makes its debut. Wuhan Coronavirus scores  out of  N masks.\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstopwords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstemmer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstemmer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-94e9df6963be>\u001b[0m in \u001b[0;36m_tokenize\u001b[0;34m(text, stopwords)\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mword_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mis_iterable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mword_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    140\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mtype\u001b[0m \u001b[0mpreserve_line\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m     \"\"\"\n\u001b[0;32m--> 142\u001b[0;31m     \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpreserve_line\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m     return [\n\u001b[1;32m    144\u001b[0m         \u001b[0mtoken\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentences\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_treebank_word_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mparam\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mPunkt\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \"\"\"\n\u001b[0;32m--> 119\u001b[0;31m     \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_punkt_tokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36m_get_punkt_tokenizer\u001b[0;34m(language)\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mtype\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \"\"\"\n\u001b[0;32m--> 105\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mPunktTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, lang)\u001b[0m\n\u001b[1;32m   1742\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"english\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1743\u001b[0m         \u001b[0mPunktSentenceTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1744\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_lang\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1745\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mload_lang\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"english\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36mload_lang\u001b[0;34m(self, lang)\u001b[0m\n\u001b[1;32m   1747\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfind\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1749\u001b[0;31m         \u001b[0mlang_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"tokenizers/punkt_tab/{lang}/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1750\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_punkt_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1751\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lang\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    577\u001b[0m     \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"*\"\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m70\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"\\n{sep}\\n{msg}\\n{sep}\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 579\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    580\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "data = data[data.columns.drop(list(data.filter(regex='token_clean')))] # we always append\n",
        "data = parallelize_dataframe(data, _normalize, field_read=\"cleaned_tweet\", field_write=\"token_clean\", stopwords=stopwords, stemmer=None, lemmanizer=None, lemma_with_ner=False)"
      ],
      "metadata": {
        "id": "xkDKdTYtnXDa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's use the library"
      ],
      "metadata": {
        "id": "UpJN9vvs1AFv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "from fhnw.nlp.utils.normalize import normalize, tokenize, tokenize_lemma, tokenize_stem\n",
        "\n",
        "data = data[data.columns.drop(list(data.filter(regex='token_clean')))] # we always append\n",
        "data = parallelize_dataframe(data, normalize, field_read=\"cleaned_tweet\", field_write=\"token_clean\", stopwords=stopwords, stemmer=None, lemmanizer=None, lemma_with_ner=False)"
      ],
      "metadata": {
        "id": "3VG0JqxqtoOC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "yep, also working\n",
        "\n",
        "Get the (lowercased) tokens first without removing stopwords if we have to come up with a reasonable set of stopwords."
      ],
      "metadata": {
        "id": "rxwpFUnW1wc3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "data = parallelize_dataframe(data, normalize, field_read=\"cleaned_tweet\", field_write=\"token_no_stopwords_clean\", stopwords=empty_stopwords, stemmer=None, lemmanizer=None, lemma_with_ner=False)"
      ],
      "metadata": {
        "id": "0NgUsKCT1pdo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We also later need text instead of tokens"
      ],
      "metadata": {
        "id": "D2-gDsdS2OCU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "data = parallelize_dataframe(data, join_tokens, field_read=\"token_clean\", field_write=\"text_clean\", stopwords=empty_stopwords)"
      ],
      "metadata": {
        "id": "5v6ld2gE2QWf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.head(3)"
      ],
      "metadata": {
        "id": "MkB3zllh2DuB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Store the work so far (so we could resume if needed)."
      ],
      "metadata": {
        "id": "TwLrQqv94L3z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "\n",
        "if (DEV):\n",
        "  # Mount Google Drive\n",
        "  drive.mount('/content/drive')\n",
        "  output_file_path = \"/content/drive/MyDrive/COVIDSenti_tokenized_01.parq\"  # save to github\n",
        "\n",
        "  # Save the DataFrame to Parquet format\n",
        "  data.to_parquet(output_file_path)\n",
        "\n",
        "  save_dataframe(data, \"data/COVIDSenti_tokenized_01.parq\")"
      ],
      "metadata": {
        "id": "q-AOJ62u3Yy5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we reload if we work from here"
      ],
      "metadata": {
        "id": "feuGufkN-x_2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if 'data' not in locals() or DEV: #check if data is defined and not empty\n",
        "  print(\"Data is empty or DEV is True\")\n",
        "  # Load from github\n",
        "  download(\"https://github.com/miam-bonbon/assignment-adv-nlp/raw/refs/heads/main/data/COVIDSenti_tokenized_01.parq\", \"data/COVIDSenti_tokenized_01.parq\")\n",
        "  data = load_dataframe(\"data/COVIDSenti_tokenized_01.parq\")\n",
        "  print(data.shape)"
      ],
      "metadata": {
        "id": "2VbAKsHp-2L2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.head()"
      ],
      "metadata": {
        "id": "eKlW83K2AH9k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's see how frequent the words (and their n-grams) are"
      ],
      "metadata": {
        "id": "aPupL-F65gZs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "def plot_ngram_counts(counter, n_most_common, title=\"Term frequencies\"):\n",
        "    \"\"\"Plots the n-gram counts\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    counter : Counter\n",
        "        The counter of the n-grams\n",
        "    n_most_common : int\n",
        "        The n most common n-grams to plot\n",
        "    title : str\n",
        "        The title of the plot\n",
        "    \"\"\"\n",
        "\n",
        "    import matplotlib.pyplot as plt\n",
        "\n",
        "    y = [count for tag, count in counter.most_common(n_most_common)]\n",
        "    x = [tag for tag, count in counter.most_common(n_most_common)]\n",
        "\n",
        "    plt.bar(x, y)\n",
        "    plt.title(title)\n",
        "    plt.ylabel(\"Frequency\")\n",
        "    #plt.yscale('log') # set log scale for y-axis\n",
        "    plt.xticks(rotation=90)\n",
        "    for i, (tag, count) in enumerate(counter.most_common(n_most_common)):\n",
        "        plt.text(i, count, f' {count} ', rotation=90, ha='center', va='top' if i < 10 else 'bottom', color='white' if i < 10 else 'black')\n",
        "    plt.xlim(-0.6, len(x)-0.4) # set tighter x lims\n",
        "    plt.tight_layout() # change the whitespace such that all labels fit nicely\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "SoeQK2-x3ZKE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from fhnw.nlp.utils.text import create_ngram_counts\n",
        "\n",
        "for label in data[\"label\"].unique():\n",
        "  ngrams_1 = create_ngram_counts(data.loc[(data[\"label\"] == label)], 1, \"token_clean\")\n",
        "  ngrams_2 = create_ngram_counts(data.loc[(data[\"label\"] == label)], 2, \"token_clean\")\n",
        "\n",
        "  plot_ngram_counts(ngrams_1, 20, \"Unigram Term frequencies of \"+label)\n",
        "  plot_ngram_counts(ngrams_2, 20, \"Bigram Term frequencies of \"+label)"
      ],
      "metadata": {
        "id": "9_Oi80B-26BJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stopwords look alright"
      ],
      "metadata": {
        "id": "kc37psm-f-9W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Extend the stopwords list\n",
        "# additional_stopwords = {\"example\", \"word\", \"anotherword\"}  # Add your specific words here\n",
        "# stopwords.update(additional_stopwords)\n",
        "\n",
        "# # Optionally, remove words from the stopword list that are helpful for distinguishing classes\n",
        "# words_to_remove = {\"distinguishing\", \"word1\", \"word2\"}  # Add words to remove\n",
        "# stopwords -= words_to_remove"
      ],
      "metadata": {
        "id": "FfLplG5j5wmq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "But let's optimize for tweets, we hava a tweet tokenizer in nltk - and keep punctuation for now"
      ],
      "metadata": {
        "id": "R0gVTsgsFbwk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lost stopwords from reload"
      ],
      "metadata": {
        "id": "5__TAY-FqWYr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "\n",
        "if ('stopwords' not in locals()):\n",
        "  !pip install 'spacy>=3.0.5'\n",
        "  !pip install nltk\n",
        "  from nltk.corpus import stopwords\n",
        "  from nltk.stem.snowball import SnowballStemmer\n",
        "\n",
        "  import spacy\n",
        "  !python3 -m spacy download en_core_web_md\n",
        "\n",
        "  nlp = spacy.load(\"en_core_web_md\")\n",
        "\n",
        "  import nltk\n",
        "  nltk.download('punkt')\n",
        "  nltk.download('stopwords')\n",
        "\n",
        "  stemmer = SnowballStemmer(\"english\")\n",
        "  stopwords = set(stopwords.words(\"english\"))\n",
        "  #stopwords = set(nlp.Defaults.stop_words)\n",
        "  empty_stopwords = set()"
      ],
      "metadata": {
        "id": "bv9jpy1jqdiS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "nltk.download('twitter_samples')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "metadata": {
        "id": "IKYBGuQBZSbf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import TweetTokenizer\n",
        "tweet_tokenizer = TweetTokenizer()\n",
        "\n",
        "def tweet_tokenize(text, stopwords):\n",
        "    \"\"\"Tokenizes and lowercases a text and removes stopwords\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    text : str, iterable\n",
        "        The text either as string or iterable of tokens (in this case tokenization is not applied)\n",
        "    stopwords : set\n",
        "        A set of stopword to remove from the tokens\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    list\n",
        "        The tokenized text\n",
        "    \"\"\"\n",
        "    from fhnw.nlp.utils.processing import is_iterable\n",
        "\n",
        "    if isinstance(text, str):\n",
        "      word_tokens = tweet_tokenizer.tokenize(text)\n",
        "    elif is_iterable(text):\n",
        "        word_tokens = text\n",
        "    else:\n",
        "        raise TypeError(\"Only string or iterable (e.g. list) is supported. Received a \"+ str(type(text)))\n",
        "\n",
        "    return [word.lower() for word in word_tokens if word.lower() not in stopwords]\n",
        "\n",
        "\n",
        "def tweet_tokenize_stem(text, stopwords, stemmer):\n",
        "    \"\"\"Tokenizes, lowercases and stems a text and removes stopwords\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    text : str, iterable\n",
        "        The text either as string or iterable of tokens (in this case tokenization is not applied)\n",
        "    stopwords : set\n",
        "        A set of stopword to remove from the tokens\n",
        "    stemmer: stemmer\n",
        "        The stemmer to use (e.g. SnowballStemmer)\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    list\n",
        "        The tokenized and stemmed text\n",
        "    \"\"\"\n",
        "    from fhnw.nlp.utils.processing import is_iterable\n",
        "\n",
        "    if isinstance(text, str):\n",
        "      word_tokens = tweet_tokenizer.tokenize(text)\n",
        "    elif is_iterable(text):\n",
        "        word_tokens = text\n",
        "    else:\n",
        "        raise TypeError(\"Only string or iterable (e.g. list) is supported. Received a \"+ str(type(text)))\n",
        "\n",
        "    # if not isinstance(stemmer, Callable):\n",
        "    stemmer = stemmer.stem\n",
        "\n",
        "    return [stemmer(word.lower()) for word in word_tokens if word.lower() not in stopwords]\n",
        "\n",
        "\n",
        "def tweet_tokenize_lemma(text, stopwords, lemmanizer, keep_ners=False):\n",
        "    \"\"\"Tokenizes, lowercases and lemmatizes a text and removes stopwords\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    text : str, iterable\n",
        "        The text either as string or iterable of tokens (in this case tokenization is not applied)\n",
        "    stopwords : set\n",
        "        A set of stopword to remove from the tokens\n",
        "    lemmanizer: spacy nlp pipeline\n",
        "        The lemmanizer to use (must be spacy nlp pipeline)\n",
        "    keep_ner: bool\n",
        "        Defines if named entities (NERs) should be keept in one token\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    list\n",
        "        The tokenized and lemmatized text\n",
        "    \"\"\"\n",
        "    from fhnw.nlp.utils.processing import is_iterable\n",
        "\n",
        "    if isinstance(text, str):\n",
        "        text = text\n",
        "    elif is_iterable(text):\n",
        "        from fhnw.nlp.utils.text import join_tokens\n",
        "        text = join_tokens(text, set())\n",
        "    else:\n",
        "        raise TypeError(\"Only string or iterable (e.g. list) is supported. Received a \"+ str(type(text)))\n",
        "\n",
        "    if keep_ners:\n",
        "        # HanoverTagger could be an alternative but takes longer\n",
        "        # see: https://textmining.wp.hs-hannover.de/Preprocessing.html#Lemmatisierung-und-Wortarterkennung\n",
        "        doc = lemmanizer(text, disable=['tagger', 'parser'])\n",
        "\n",
        "        tokens = list()\n",
        "        ner_idx = 0\n",
        "        tok_idx = 0\n",
        "\n",
        "        # keep ner in one token\n",
        "        while tok_idx < len(doc):\n",
        "            if ner_idx >= len(doc.ents) or doc[tok_idx].idx < doc.ents[ner_idx].start_char:\n",
        "                if doc[tok_idx].is_alpha and not doc[tok_idx].is_punct and doc[tok_idx].text.lower() not in stopwords and doc[tok_idx].lemma_.lower() not in stopwords:\n",
        "                    #print(\"token \", doc[tok_idx].lemma_.lower())\n",
        "                    tokens.append(doc[tok_idx].lemma_.lower())\n",
        "\n",
        "                tok_idx += 1\n",
        "            else:\n",
        "                #print(\"ner \", doc.ents[ner_idx].lemma_.lower())\n",
        "                tokens.append(doc.ents[ner_idx].lemma_.lower())\n",
        "\n",
        "                tok_idx += 1\n",
        "                while tok_idx < len(doc) and doc[tok_idx].idx < doc.ents[ner_idx].end_char:\n",
        "                    tok_idx += 1\n",
        "\n",
        "                ner_idx += 1\n",
        "\n",
        "        return tokens\n",
        "    else:\n",
        "        doc = lemmanizer(text, disable=['tagger', 'parser', 'ner'])\n",
        "        return [tok.lemma_.lower() for tok in doc if tok.is_alpha and not tok.is_punct and tok.text.lower() not in stopwords and tok.lemma_.lower() not in stopwords]\n",
        "\n",
        "\n",
        "def _tweet_normalize(text, stopwords, stemmer=None, lemmanizer=None, lemma_with_ner=False):\n",
        "    \"\"\"Normalizes (e.g. tokenize and stem) and lowercases a text and removes stopwords\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    text : str, iterable\n",
        "        The text either as string or iterable of tokens (in this case tokenization is not applied)\n",
        "    stopwords : set\n",
        "        A set of stopword to remove from the tokens\n",
        "    stemmer: stemmer\n",
        "        The stemmer to use (e.g. SnowballStemmer) or None to disable stemming\n",
        "    lemmanizer: spacy nlp pipeline\n",
        "        The lemmanizer to use (must be spacy nlp pipeline) or None to disable lemmantization\n",
        "    lemma_with_ner: bool\n",
        "        Defines if named entities (NERs) should be keept in one token\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    list\n",
        "        The normalized text\n",
        "    \"\"\"\n",
        "\n",
        "    if lemmanizer is not None:\n",
        "        return tweet_tokenize_lemma(text, stopwords, lemmanizer, keep_ners=lemma_with_ner)\n",
        "    elif stemmer is not None:\n",
        "        return tweet_tokenize_stem(text, stopwords, stemmer)\n",
        "    else:\n",
        "        return tweet_tokenize(text, stopwords)"
      ],
      "metadata": {
        "id": "YJCth3OJVDkx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's use the the tweet_tokenize"
      ],
      "metadata": {
        "id": "Ne1HrJZUXHRw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "data = data[data.columns.drop(list(data.filter(regex='token_clean_tweet_tokenize')))] # we always append\n",
        "data = parallelize_dataframe(data, _tweet_normalize, field_read=\"cleaned_tweet\", field_write=\"token_clean_tweet_tokenize\", stopwords=stopwords, stemmer=None, lemmanizer=None, lemma_with_ner=False)"
      ],
      "metadata": {
        "id": "Srrf_kmPXHRx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.head(3)"
      ],
      "metadata": {
        "id": "6y8bP4e4XHRz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check the n-grams for tweet optimized"
      ],
      "metadata": {
        "id": "Fe1ol-_ziUih"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from fhnw.nlp.utils.text import create_ngram_counts\n",
        "\n",
        "for label in data[\"label\"].unique():\n",
        "  ngrams_1 = create_ngram_counts(data.loc[(data[\"label\"] == label)], 1, \"token_clean_tweet_tokenize\")\n",
        "  ngrams_2 = create_ngram_counts(data.loc[(data[\"label\"] == label)], 2, \"token_clean_tweet_tokenize\")\n",
        "\n",
        "  plot_ngram_counts(ngrams_1, 20, \"Unigram Term frequencies of tweet optimized \"+label)\n",
        "  plot_ngram_counts(ngrams_2, 20, \"Bigram Term frequencies of tweet optimized \"+label)"
      ],
      "metadata": {
        "id": "M4jjbUPBiTOL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nice! We keep our hashtags."
      ],
      "metadata": {
        "id": "zVA7tnW_ioFy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "\n",
        "if (DEV):\n",
        "  # Mount Google Drive\n",
        "  drive.mount('/content/drive')\n",
        "  output_file_path = \"/content/drive/MyDrive/COVIDSenti_tokenized_02.parq\"  # save to github\n",
        "\n",
        "  # Save the DataFrame to Parquet format\n",
        "  data.to_parquet(output_file_path)\n",
        "\n",
        "  save_dataframe(data, \"data/COVIDSenti_tokenized_02.parq\")"
      ],
      "metadata": {
        "id": "NdMBIjMKij5u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "l9nUf02BoKKZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}